{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brick Loaded\n"
     ]
    }
   ],
   "source": [
    "#essential libraries\n",
    "\n",
    "#from google_drive import gdrive\n",
    "\n",
    "from matplotlib.pyplot import show\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import *\n",
    "import operator\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import shelve\n",
    "import re\n",
    "from collections import Counter, defaultdict, OrderedDict, deque\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy, copy\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "import os\n",
    "import gc\n",
    "import linecache\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from rdflib import URIRef\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy import spatial\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy import stats\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "from scipy.spatial.distance import cosine as cosine_similarity\n",
    "\n",
    "from divergence import gau_js as js_divergence\n",
    "#from brick_parser import tagList, tagsetList, equipTagsetList, pointTagsetList, locationTagsetList,\\\n",
    "#pointTagList, equipTagList, locationTagList, equipPointDict\n",
    "from brick_parser import equipTagsetList, pointTagsetList, locationTagsetList\n",
    "tagsetList = list(set(equipTagsetList + pointTagsetList + locationTagsetList))\n",
    "adder = lambda x,y: x + y\n",
    "tagList = list(set(reduce(adder, [tagset.split('_') for tagset in tagsetList])))\n",
    "\n",
    "#from cmu_parser import cmu_building_parse\n",
    "\n",
    "debugFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#building_name = 'ap_m'\n",
    "#building_name = 'bml'\n",
    "#building_name = 'ghc'\n",
    "#building_name = 'ebu3b'\n",
    "building_name = 'pouya'\n",
    "notUcsdBuildings = ['ghc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e55ee193696f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeledFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#truthDF = pd.read_excel(fp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtruthDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#truthDF = truthDF.set_index(keys='Unique Identifier')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import building_tokenizer\n",
    "imp.reload(building_tokenizer)\n",
    "import building_tokenizer as toker\n",
    "from functools import reduce\n",
    "\n",
    "tokenType = 'NoNumber'\n",
    "if not building_name in notUcsdBuildings:\n",
    "    naeDict = dict()\n",
    "    naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "    naeDict['ap_m'] = ['514', '513','604']\n",
    "    naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "    naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "    naeDict['bml'] = [\"510\"]\n",
    "    #naeDict['otterson'] = [\"518\", \"517\", \"589\", \"590\"]\n",
    "    naeList = naeDict[building_name]\n",
    "\n",
    "    labeledFile = 'metadata/' + building_name + '_sensor_types_location.csv'\n",
    "    with open(labeledFile, 'rb') as fp:\n",
    "        #truthDF = pd.read_excel(fp)\n",
    "        truthDF = pd.DataFrame.from_csv(fp)\n",
    "        #truthDF = truthDF.set_index(keys='Unique Identifier')\n",
    "\n",
    "    wordFeatFile = 'data/wordfeat_'+building_name+'.pkl'\n",
    "\n",
    "    tokenTypeList = ['NoNumber', 'Alphanumeric', 'AlphaAndNum', 'NumAsSingleWord']\n",
    "\n",
    "    bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "    unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "    for val in Counter(unitMap.keys()).values():\n",
    "        if val>1:\n",
    "            \"Unit map file ERROR\"\n",
    "            assert(False)\n",
    "            \n",
    "            \n",
    "    trueDF = pd.DataFrame.from_csv('metadata/'+building_name+'_sensor_types_location.csv')\n",
    "    sensorDF, nameList, jcinameList, descList, unitList, bacnettypeList, wordList = \\\n",
    "    toker.structure_metadata(buildingName=building_name, tokenType=tokenType, \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "\n",
    "    origSensorDF = deepcopy(sensorDF)\n",
    "    origNameList = deepcopy(nameList)\n",
    "    origJcinameList = deepcopy(jcinameList)\n",
    "    origDescList = deepcopy(descList)\n",
    "    origUnitList = deepcopy(unitList)\n",
    "    origBacnettypeList = deepcopy(bacnettypeList)\n",
    "    origWordList = deepcopy(wordList)\n",
    "\n",
    "    #_, rawNameList, rawJcinameList, rawDescList, _, _, _ = toker.structure_metadata(building_name=building_name, tokenType='AlphaAndNum', \\\n",
    "    _, rawNameList, rawJcinameList, rawDescList, _, _, _ =\\\n",
    "    toker.structure_metadata(buildingName=building_name, tokenType='JustSeparate', \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "    \n",
    "    \n",
    "else: \n",
    "    filename = 'metadata/'+building_name+'_sensor_types_location.csv'\n",
    "    #   filename = 'metadata/%s_sensor_types_location.csv'%building_name\n",
    "    df = pd.read_csv(filename)\n",
    "    sentenceDict = dict()    \n",
    "    for i,raw in enumerate(df['bas_raw'].tolist()):\n",
    "        sentenceDict[i] = toker.tokenize(tokenType, raw)\n",
    "    \n",
    "    \n",
    "## Common part\n",
    "    \n",
    "#with open('data/'+building_name+'_str_score_dict.pkl', 'rb') as fp:\n",
    "#    scoreDictDict = pickle.load(fp)\n",
    "#    ### If a word is exactly matched with another, fix it.\n",
    "#    for word, scoreDict in scoreDictDict.items():\n",
    "#        if word in scoreDict.keys():\n",
    "#            scoreDictDict[word] = {word:1}\n",
    "#    scoreDictDict['co'] = {'co2':1}\n",
    "        \n",
    "adder = lambda x,y:x+y\n",
    "splitter = lambda x:x.split()\n",
    "bacnetTypes = list(set(reduce(adder,map(splitter,origBacnettypeList),[])))\n",
    "#for bacnetType in bacnetTypes:\n",
    "#    scoreDictDict[bacnetType] = {bacnetType:1}\n",
    "#units = list(set(unitMap['word'].tolist()))\n",
    "#for unit in units:\n",
    "#    if type(unit)==str:\n",
    "#        scoreDictDict[unit] = {unit:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"metadata/%s_sentence_dict_justseparate.json\"%building_name, \"r\") as fp:\n",
    "    sentenceDict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('metadata/word_dict.json', 'r') as fp:\n",
    "    known_word_dict = json.load(fp)[building_name]\n",
    "known_word_list = known_word_dict.keys()\n",
    "with open('metadata/re_dict.json', 'r') as fp:\n",
    "    re_dict = json.load(fp)[building_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nae': 'networkadapter-nae'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_re_rules(word, phrase, re_dict=re_dict):\n",
    "    re_list = re_dict.get(word)\n",
    "    if re_list:\n",
    "        for (re_rule, label) in re_list:\n",
    "            if re.findall(re_rule, phrase):\n",
    "                return label\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_label(label):\n",
    "    nonBrickTagList = ['none', 'rightidentifier', 'leftidentifier', 'pump_flow_status',\n",
    "                       'networkadapter', 'vnd', 'name']\n",
    "    # name: stand alone unique name.\n",
    "    # left identifier: contraints meaning of left tagset\n",
    "    # right identifier: contraints meaning of right tagset\n",
    "    label = label.split('-')[0]\n",
    "    if label in nonBrickTagList:\n",
    "        return True\n",
    "    token_list = label.split('_')\n",
    "    valid_label_list = tagList + tagsetList\n",
    "    for token in token_list:\n",
    "        valid_label_list = [label for label in valid_label_list if token in label]\n",
    "    if len(valid_label_list)>0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "\n",
    "def ask_expert_labels(sentence):\n",
    "    # Input: List of words\n",
    "    # Output: feature list:\n",
    "    # Human operation: Give labels on the words.\n",
    "    sliceSize = 2\n",
    "    featureDict = dict()\n",
    "\n",
    "    #for words in rolling_window(sentence, 2):\n",
    "    #    for word in wo\n",
    "    print(\"\\nGiven sentence: %s\"%sentence)\n",
    "    labelList = list()\n",
    "    for i, word in enumerate(sentence):\n",
    "        if re.match('[^0-9a-zA-Z]+', word):\n",
    "            labelList.append('none')\n",
    "            continue\n",
    "        if re.match('\\d+', word):\n",
    "            labelList.append('leftidentifier')\n",
    "            continue\n",
    "        if word in known_word_list:\n",
    "            labelList.append(known_word_dict[word])\n",
    "            continue\n",
    "        start_idx = max(i-2, 0)\n",
    "        end_idx = min(i+3, len(sentence))\n",
    "        phrase = ''.join(sentence[start_idx:end_idx])\n",
    "        found_by_re = check_re_rules(word, phrase, re_dict=re_dict)\n",
    "        if found_by_re:\n",
    "            labelList.append(found_by_re)\n",
    "            continue\n",
    "        if word in tagList:\n",
    "            labelList.append(word)\n",
    "            continue\n",
    "        while True:\n",
    "            label = input(str(word)+': \\n')\n",
    "            if validate_label(label):\n",
    "                labelList.append(label)\n",
    "                break\n",
    "            else:\n",
    "                print(\"Pick Tag among the following tags: \" + str(tagList))\n",
    "    \n",
    "    # compensate left->right_identifier of numbers with rd, st, th\n",
    "    for i, (word, label) in enumerate(zip(sentence, labelList)):\n",
    "        if i < len(sentence)-1:\n",
    "            if word.isdigit() and sentence[i+1] in ['rd', 'nd', 'st', 'th']:\n",
    "                labelList[i] = 'rightidentifier'           \n",
    "    \n",
    "    return labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelFilename = 'metadata/%s_label_dict_justseparate.json'%building_name\n",
    "if os.path.isfile(labelFilename):\n",
    "    with open(labelFilename, 'r') as fp:\n",
    "        labelListDict = json.load(fp)\n",
    "else:\n",
    "    labelListDict = {}\n",
    "with open(labelFilename+'.backup', 'w') as fp:\n",
    "    json.dump(labelListDict, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NAE_04.NAE~2d04~2fFC~2d2~2eFD~2d52-VAV~2d21~2d04~2eVAV~2d21~2d04-DA~2dVP~2eTrend1\n",
      "\n",
      "Given sentence: ['_', 'nae', '_', '04', '.', 'nae', '~', '2', 'd', '04', '~', '2', 'ffc', '~', '2', 'd', '2', '~', '2', 'efd', '~', '2', 'd', '52', '-', 'vav', '~', '2', 'd', '21', '~', '2', 'd', '04', '~', '2', 'e', 'vav', '~', '2', 'd', '21', '~', '2', 'd', '04', '-', 'da', '~', '2', 'dvp', '~', '2', 'etrend', '1', '\\n', '\\n', '\\n']\n",
      "d: \n",
      "leftidentifier\n",
      "ffc: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "efd: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "e: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "da: \n",
      "discharge_air\n",
      "dvp: \n",
      "none\n",
      "etrend: \n",
      "none\n",
      "_NAE_04.NAE~2d04~2fFC~2d1~2eFD~2d67-VAV~2d14~2d08~2eVAV~2d14~2d08-UNIT~2dENA~2eTrend1\n",
      "\n",
      "Given sentence: ['_', 'nae', '_', '04', '.', 'nae', '~', '2', 'd', '04', '~', '2', 'ffc', '~', '2', 'd', '1', '~', '2', 'efd', '~', '2', 'd', '67', '-', 'vav', '~', '2', 'd', '14', '~', '2', 'd', '08', '~', '2', 'e', 'vav', '~', '2', 'd', '14', '~', '2', 'd', '08', '-', 'unit', '~', '2', 'dena', '~', '2', 'etrend', '1', '\\n', '\\n', '\\n']\n",
      "d: \n",
      "leftidentifier\n",
      "ffc: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "efd: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "e: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "dena: \n",
      "enable\n",
      "etrend: \n",
      "none\n",
      "_NAE_04.NAE~2d04~2fFC~2d1~2eFD~2d45-VAV~2d18~2dP3~2eVAV~2d18~2dP3-UNIT~2dENA~2eTrend1\n",
      "\n",
      "Given sentence: ['_', 'nae', '_', '04', '.', 'nae', '~', '2', 'd', '04', '~', '2', 'ffc', '~', '2', 'd', '1', '~', '2', 'efd', '~', '2', 'd', '45', '-', 'vav', '~', '2', 'd', '18', '~', '2', 'dp', '3', '~', '2', 'e', 'vav', '~', '2', 'd', '18', '~', '2', 'dp', '3', '-', 'unit', '~', '2', 'dena', '~', '2', 'etrend', '1', '\\n', '\\n', '\\n']\n",
      "d: \n",
      "leftidentifier\n",
      "ffc: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "efd: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "dp: \n",
      "none\n",
      "e: \n",
      "leftidentifier\n",
      "d: \n",
      "leftidentifier\n",
      "dp: \n",
      "leftidentifier\n",
      "dena: \n",
      "enable\n",
      "etrend: \n",
      "none\n",
      "_NAE_04.NAE~2d04~2fFC~2d1~2eFD~2d36-VAV~2d18~2d11~2eVAV~2d18~2d11-DA~2dVP~2eTrend1\n",
      "\n",
      "Given sentence: ['_', 'nae', '_', '04', '.', 'nae', '~', '2', 'd', '04', '~', '2', 'ffc', '~', '2', 'd', '1', '~', '2', 'efd', '~', '2', 'd', '36', '-', 'vav', '~', '2', 'd', '18', '~', '2', 'd', '11', '~', '2', 'e', 'vav', '~', '2', 'd', '18', '~', '2', 'd', '11', '-', 'da', '~', '2', 'dvp', '~', '2', 'etrend', '1', '\\n', '\\n', '\\n']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/jbkoh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jbkoh/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jbkoh/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jbkoh/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6595d665fb0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msrcid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabelListDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mlabelListDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrcid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_expert_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrcid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-567bea42b551>\u001b[0m in \u001b[0;36mask_expert_labels\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m': \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mlabelList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jbkoh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jbkoh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labelSampleNum = 200\n",
    "\n",
    "srcids = sentenceDict.keys()\n",
    "if building_name == 'ghc':\n",
    "    for srcid in random.sample(srcids, labelSampleNum):\n",
    "        if not srcid in labelListDict.keys():\n",
    "            print(srcid)\n",
    "            labelListDict[srcid] = ask_expert_labels(sentenceDict[srcid])\n",
    "else:    \n",
    "    #for i in random.sample(range(0,len(sensorDF)), labelSampleNum):\n",
    "        #srcid = sensorDF.iloc[i].name\n",
    "    for srcid in random.sample(srcids, labelSampleNum):\n",
    "        if not srcid in labelListDict.keys():\n",
    "            print(srcid)\n",
    "            labelListDict[srcid] = ask_expert_labels(sentenceDict[srcid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(labelListDict))\n",
    "with open(labelFilename, 'w') as fp:\n",
    "    json.dump(labelListDict,fp, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrongSrcidList = list()\n",
    "for srcid, sentence in sentenceDict.items():\n",
    "    if 't' in sentence and '5' in sentence:\n",
    "        #wrongSrcidList.append(srcid)\n",
    "        if srcid in labelListDict.keys():\n",
    "            print(srcid, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
