{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import building_tokenizer\n",
    "reload(building_tokenizer)\n",
    "import building_tokenizer as toker\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildingName = 'ebu3b'\n",
    "notUcsdBuildings = ['ghc']\n",
    "\n",
    "tokenType = 'NoNumber'\n",
    "\n",
    "if not buildingName in notUcsdBuildings:\n",
    "    naeDict = dict()\n",
    "    naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "    naeDict['ap_m'] = ['514', '513','604']\n",
    "    naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "    naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "    #naeDict['otterson'] = [\"518\", \"517\", \"589\", \"590\"]\n",
    "    naeList = naeDict[buildingName]\n",
    "\n",
    "    labeledFile = 'metadata/' + buildingName + '_sensor_types_location.csv'\n",
    "    with open(labeledFile, 'rb') as fp:\n",
    "        #truthDF = pd.read_excel(fp)\n",
    "        truthDF = pd.DataFrame.from_csv(fp)\n",
    "        #truthDF = truthDF.set_index(keys='Unique Identifier')\n",
    "\n",
    "    wordFeatFile = 'data/wordfeat_'+buildingName+'.pkl'\n",
    "\n",
    "    tokenTypeList = ['NoNumber', 'Alphanumeric', 'AlphaAndNum', 'NumAsSingleWord']\n",
    "\n",
    "    bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "    unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "    for val in Counter(unitMap.keys()).values():\n",
    "        if val>1:\n",
    "            \"Unit map file ERROR\"\n",
    "            assert(False)\n",
    "            \n",
    "            \n",
    "    trueDF = pd.DataFrame.from_csv('metadata/'+buildingName+'_sensor_types_location.csv')\n",
    "    sensorDF, nameList, jcinameList, descList, unitList, bacnettypeList, wordList = \\\n",
    "    toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "\n",
    "    origSensorDF = deepcopy(sensorDF)\n",
    "    origNameList = deepcopy(nameList)\n",
    "    origJcinameList = deepcopy(jcinameList)\n",
    "    origDescList = deepcopy(descList)\n",
    "    origUnitList = deepcopy(unitList)\n",
    "    origBacnettypeList = deepcopy(bacnettypeList)\n",
    "    origWordList = deepcopy(wordList)\n",
    "\n",
    "    #_, rawNameList, rawJcinameList, rawDescList, _, _, _ = toker.structure_metadata(buildingName=buildingName, tokenType='AlphaAndNum', \\\n",
    "    _, rawNameList, rawJcinameList, rawDescList, _, _, _ =\\\n",
    "    toker.structure_metadata(buildingName=buildingName, tokenType='JustSeparate', \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "    \n",
    "'''\n",
    "else: \n",
    "    filename = 'metadata/'+buildingName+'_sensor_types_location.csv'\n",
    "    #   filename = 'metadata/%s_sensor_types_location.csv'%buildingName\n",
    "    df = pd.read_csv(filename)\n",
    "    sentenceDict = dict()    \n",
    "    for i,raw in enumerate(df['bas_raw'].tolist()):\n",
    "        sentenceDict[i] = toker.tokenize(tokenType, raw)\n",
    "'''     \n",
    "    \n",
    "## Common part\n",
    "    \n",
    "#with open('data/'+buildingName+'_str_score_dict.pkl', 'rb') as fp:\n",
    "#    scoreDictDict = pickle.load(fp)\n",
    "#    ### If a word is exactly matched with another, fix it.\n",
    "#    for word, scoreDict in scoreDictDict.items():\n",
    "#        if word in scoreDict.keys():\n",
    "#            scoreDictDict[word] = {word:1}\n",
    "#    scoreDictDict['co'] = {'co2':1}\n",
    "        \n",
    "adder = lambda x,y:x+y\n",
    "splitter = lambda x:x.split()\n",
    "bacnetTypes = list(set(reduce(adder,map(splitter,origBacnettypeList),[])))\n",
    "#for bacnetType in bacnetTypes:\n",
    "#    scoreDictDict[bacnetType] = {bacnetType:1}\n",
    "#units = list(set(unitMap['word'].tolist()))\n",
    "#for unit in units:\n",
    "#    if type(unit)==str:\n",
    "#        scoreDictDict[unit] = {unit:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitter = lambda x:x.split(' ')\n",
    "sentenceDict = dict()\n",
    "for i, row in enumerate(zip(rawNameList, rawJcinameList, rawDescList)):\n",
    "    para = list()\n",
    "    for ele in row:\n",
    "        para.extend([x for x in ele.split(' ') if x!=''])\n",
    "        para.append('newline')\n",
    "    sentenceDict[sensorDF.iloc[i].name] = para\n",
    "with open(\"metadata/%s_sentence_dict.json\"%buildingName, \"w\") as fp:\n",
    "    json.dump(sentenceDict, fp)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
