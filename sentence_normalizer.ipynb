{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import building_tokenizer as toker\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#naeListDict['Applied_Physics_and_Mathematics'] = ['514', '513', '604']\n",
    "#naeListDict['Computer_Science_and_Engineering_Building'] = ['505', '506']\n",
    "#naeListDict['Conrad_Prebys_Music_Center'] = ['523']\n",
    "#naeListDict['San_Diego_Supercomputer_Center'] = ['668', '575', '524', '520']\n",
    "#naeListDict['Literature_Building'] = ['553']\n",
    "#naeListDict['Structural_and_Materials_Engineering_Building'] = ['572', '573', '574']\n",
    "#naeListDict['Jacobs_Hall'] = ['620', '621', '622', '623', '650', '639', '638', '642', '643', '640', \\\n",
    "#                              '641', '637', '646', '644', '645']\n",
    "#naeListDict['Warren_Lecture_Hall'] = ['555', '552']\n",
    "#naeDict = dict()\n",
    "#naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "#naeDict['ap_m'] = ['514', '513','604']\n",
    "#naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "#naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "#naeDict['music'] = ['523']\n",
    "#naeDict['sme'] = ['572', '573', '574']\n",
    "#naeDict['otterson'] = [\"518\", \"517\", \"589\", \"590\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_map_dict = dict()\n",
    "token_map_dict['ap_m'] = {\n",
    "    'vavb': ['vav', 'b'],\n",
    "    'lvfd': ['l', 'vfd'],\n",
    "    'rvfd': ['r', 'vfd'],\n",
    "    'bsmtsf': ['bsmt', 'sf'],\n",
    "    'lvlco': ['lvl', 'co']\n",
    "}\n",
    "token_map_dict['ebu3b'] = dict()\n",
    "\n",
    "token_map = token_map_dict['ap_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_tokenize(word):\n",
    "    if token_map.get(word):\n",
    "        return token_map[word]\n",
    "    else:\n",
    "        return [word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildingNameList = ['ebu3b', 'sme', 'ap_m', 'music']\n",
    "notUcsdBuildings = ['ghc']\n",
    "\n",
    "adder = lambda x,y:x+y\n",
    "\n",
    "\n",
    "for buildingName in buildingNameList:\n",
    "    for tokenType in ['AlphaAndNum', 'JustSeparate']:\n",
    "\n",
    "        if not buildingName in notUcsdBuildings:\n",
    "\n",
    "            bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "            unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "            for val in Counter(unitMap.keys()).values():\n",
    "                if val>1:\n",
    "                    \"Unit map file ERROR\"\n",
    "                    assert(False)\n",
    "\n",
    "            #sensorDF, nameList, jcinameList, descList, unitList, bacnettypeList, wordList = \\\n",
    "            #toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, withDotFlag=False)\n",
    "\n",
    "            sensorDF, rawNameList, rawJcinameList, rawDescList, _, _, _ =\\\n",
    "            toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, withDotFlag=False)\n",
    "            #toker.structure_metadata(buildingName=buildingName, tokenType='JustSeparate', withDotFlag=False)\n",
    "\n",
    "        '''\n",
    "        else: \n",
    "            filename = 'metadata/'+buildingName+'_sensor_types_location.csv'\n",
    "            #   filename = 'metadata/%s_sensor_types_location.csv'%buildingName\n",
    "            df = pd.read_csv(filename)\n",
    "            sentenceDict = dict()    \n",
    "            for i,raw in enumerate(df['bas_raw'].tolist()):\n",
    "                sentenceDict[i] = toker.tokenize(tokenType, raw)\n",
    "        '''     \n",
    "\n",
    "        #adder = lambda x,y:x+y\n",
    "        #splitter = lambda x:x.split()\n",
    "        #bacnetTypes = list(set(reduce(adder,map(splitter,origBacnettypeList),[])))\n",
    "\n",
    "        ## Common part\n",
    "\n",
    "        #with open('data/'+buildingName+'_str_score_dict.pkl', 'rb') as fp:\n",
    "        #    scoreDictDict = pickle.load(fp)\n",
    "        #    ### If a word is exactly matched with another, fix it.\n",
    "        #    for word, scoreDict in scoreDictDict.items():\n",
    "        #        if word in scoreDict.keys():\n",
    "        #            scoreDictDict[word] = {word:1}\n",
    "        #    scoreDictDict['co'] = {'co2':1}\n",
    "\n",
    "        #for bacnetType in bacnetTypes:\n",
    "        #    scoreDictDict[bacnetType] = {bacnetType:1}\n",
    "        #units = list(set(unitMap['word'].tolist()))\n",
    "        #for unit in units:\n",
    "        #    if type(unit)==str:\n",
    "        #        scoreDictDict[unit] = {unit:1}\n",
    "        splitter = lambda x:x.split(' ')\n",
    "        sentenceDict = dict()\n",
    "        charSentenceDict = dict()\n",
    "        for i, row in enumerate(zip(rawNameList, rawJcinameList, rawDescList)):\n",
    "            para = list()\n",
    "            for ele in row:\n",
    "                para.extend(reduce(adder,[word_tokenize(x) for x in ele.split(' ') if x!=''],[]))\n",
    "                para.append('\\n')\n",
    "            sentenceDict[sensorDF.iloc[i].name] = para\n",
    "            charSentenceDict[sensorDF.iloc[i].name] = list(reduce(adder, para, ''))\n",
    "        with open(\"metadata/%s_sentence_dict_%s.json\" % (buildingName, tokenType.lower()), \"w\") as fp:\n",
    "            json.dump(sentenceDict, fp, indent=2, separators=(',',':'))\n",
    "        with open(\"metadata/%s_char_sentence_dict_%s.json\" % (buildingName, tokenType.lower()), \"w\") as fp:\n",
    "            json.dump(charSentenceDict, fp, indent=2, separators=(',',':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"etc/fins_success.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Finished!!!\")\n",
    "sound_file = 'etc/fins_success.wav'\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
